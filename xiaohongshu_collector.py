"""
å°çº¢ä¹¦å¸–å­æ”¶é›†å™¨ - ä¼˜åŒ–ç‰ˆ

ç»“åˆäº†åŸç‰ˆå’Œ vibetest çš„ä¼˜ç‚¹ï¼š
1. æ€§èƒ½ä¼˜åŒ–çš„æµè§ˆå™¨é…ç½®
2. Scout æœºåˆ¶ï¼šå…ˆæ¢æµ‹åæ”¶é›†
3. å¹¶å‘æ”¶é›†ï¼ˆå¯é€‰ï¼‰
4. æ›´å¥½çš„èµ„æºç®¡ç†
5. æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†
6. å§‹ç»ˆæ˜¾ç¤ºæµè§ˆå™¨çª—å£ï¼ˆä¸ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼‰
"""

from browser_use import Agent, Browser, BrowserConfig, BrowserContextConfig
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv
import asyncio
import json
from datetime import datetime
import os
import re
from typing import List, Dict, Optional

load_dotenv()


class XiaohongshuCollectorOptimized:
    """
    å°çº¢ä¹¦å¸–å­æ”¶é›†å™¨ - ä¼˜åŒ–ç‰ˆ

    æ–°åŠŸèƒ½ï¼š
    - Scout æ¨¡å¼ï¼šå…ˆæ¢æµ‹é¡µé¢ç»“æ„
    - æ€§èƒ½ä¼˜åŒ–çš„æµè§ˆå™¨é…ç½®
    - æ”¯æŒå¹¶å‘æ”¶é›†
    - æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
    - å§‹ç»ˆæ˜¾ç¤ºæµè§ˆå™¨ï¼ˆä¾¿äºè§‚å¯Ÿå’Œè°ƒè¯•ï¼‰
    """

    def __init__(
        self,
        xiaohongshu_url: str,
        max_posts: int = 5,
        use_vision: bool = False,
        concurrent: bool = False,
        max_concurrent: int = 3
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆæ”¶é›†å™¨

        Args:
            xiaohongshu_url: å°çº¢ä¹¦é¡µé¢ URL
            max_posts: æ”¶é›†å¸–å­æ•°é‡
            use_vision: æ˜¯å¦å¯ç”¨è§†è§‰æ¨¡å¼ï¼ˆæ˜¾ç¤ºå…ƒç´ æ ‡è¯†ï¼‰
            concurrent: æ˜¯å¦å¹¶å‘æ”¶é›†
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
        """
        self.xiaohongshu_url = xiaohongshu_url
        self.max_posts = max_posts
        self.use_vision = use_vision
        self.concurrent = concurrent
        self.max_concurrent = max_concurrent

        # åˆ›å»º LLM
        self.llm = ChatGoogleGenerativeAI(
            model='gemini-2.0-flash-exp',
            temperature=0.7
        )

        # åˆ›å»ºæµè§ˆå™¨é…ç½®ï¼ˆå€Ÿé‰´ vibetest çš„æ€§èƒ½ä¼˜åŒ–ï¼Œä½†ä¸ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼‰
        self.browser_config = BrowserConfig(
            headless=False,  # å§‹ç»ˆæ˜¾ç¤ºæµè§ˆå™¨
            highlight_elements=use_vision,
            disable_security=True,
            extra_chromium_args=[
                '--disable-gpu',
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-renderer-backgrounding',
                '--disable-features=TranslateUI',
                '--no-first-run',
                '--no-default-browser-check',
            ]
        )

        self.browser = Browser(config=self.browser_config)
        self.context = None

        # è¾“å‡ºç›®å½•
        self.output_dir = "collected_posts_optimized"
        os.makedirs(self.output_dir, exist_ok=True)

    def extract_json_from_text(self, text: str, is_array: bool = False) -> Optional[Dict]:
        """ä»æ–‡æœ¬ä¸­æå– JSON"""
        # æ¨¡å¼1: <result>```json ... ```</result>
        pattern = r'<result>\s*```json\s*(\[.*?\]|\{.*?\})\s*```\s*</result>' if is_array else r'<result>\s*```json\s*(\{.*?\})\s*```\s*</result>'
        match = re.search(pattern, text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass

        # æ¨¡å¼2: ```json ... ```
        pattern = r'```json\s*(\[.*?\]|\{.*?\})\s*```' if is_array else r'```json\s*(\{.*?\})\s*```'
        match = re.search(pattern, text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass

        return None

    async def scout_posts(self) -> Dict:
        """
        Scout æ¨¡å¼ï¼šå…ˆæ¢æµ‹é¡µé¢ä¸Šçš„å¸–å­å…ƒç´ 

        è¿™æ˜¯å€Ÿé‰´ vibetest çš„æ ¸å¿ƒä¼˜åŒ–ï¼š
        å…ˆç”¨ä¸€ä¸ª Agent è¯†åˆ«æ‰€æœ‰å¸–å­å…ƒç´ ï¼Œå†åˆ†é…æ”¶é›†ä»»åŠ¡
        """
        print("ğŸ” æ­¥éª¤0: Scout - æ¢æµ‹é¡µé¢ç»“æ„...")

        scout_task = f"""
        è®¿é—® {self.xiaohongshu_url}

        è¯·è¯†åˆ«é¡µé¢ä¸Šçš„æ‰€æœ‰å¸–å­å¡ç‰‡å…ƒç´ ã€‚
        è§‚å¯Ÿå¹¶è®°å½•ï¼š
        - æ€»å…±æœ‰å¤šå°‘ä¸ªå¸–å­
        - æ¯ä¸ªå¸–å­çš„ä½ç½®å’Œæ ‡è¯†
        - å¸–å­çš„æ’åˆ—æ–¹å¼

        ä¸è¦ç‚¹å‡»ä»»ä½•å†…å®¹ï¼Œåªéœ€è¦è§‚å¯Ÿå’ŒæŠ¥å‘Šã€‚
        """

        scout_agent = Agent(
            task=scout_task,
            llm=self.llm,
            browser_context=self.context,
            use_vision=self.use_vision
        )

        scout_result = await scout_agent.run()
        scout_report = str(scout_result.final_result()) if hasattr(scout_result, 'final_result') else str(scout_result)

        print(f"âœ… Scout å®Œæˆï¼Œé¡µé¢ç»“æ„å·²è¯†åˆ«")
        print(f"ğŸ“‹ Scout æŠ¥å‘Šæ‘˜è¦: {scout_report[:200]}...\n")

        # è¿”å› scout æŠ¥å‘Šï¼Œä¾›åç»­ä½¿ç”¨
        return {"report": scout_report, "timestamp": datetime.now().isoformat()}

    async def collect_post_list(self) -> List[Dict]:
        """æ”¶é›†å¸–å­åˆ—è¡¨"""
        print("ğŸ“‹ æ­¥éª¤1: æ”¶é›†å¸–å­åˆ—è¡¨...")

        list_task = f"""
        è®¿é—® {self.xiaohongshu_url}

        ä½¿ç”¨ extract_structured_data æ”¶é›†é¡µé¢å‰ {self.max_posts} ä¸ªå¸–å­çš„ä¿¡æ¯ï¼š
        - position: åºå·ï¼ˆ1, 2, 3...ï¼‰
        - title: æ ‡é¢˜
        - author: ä½œè€…
        - likes: ç‚¹èµæ•°
        - url: é“¾æ¥ï¼ˆå¦‚æœå¯è§ï¼‰

        è¿”å› JSON æ•°ç»„æ ¼å¼
        """

        list_agent = Agent(
            task=list_task,
            llm=self.llm,
            browser_context=self.context,
            use_vision=self.use_vision
        )

        list_result = await list_agent.run()

        # æå–å¸–å­åˆ—è¡¨
        posts_list = []
        for content in list_result.extracted_content():
            extracted = self.extract_json_from_text(str(content), is_array=True)
            if extracted:
                posts_list = extracted
                break

        print(f"âœ… æ”¶é›†åˆ° {len(posts_list)} ä¸ªå¸–å­\n")
        return posts_list

    async def collect_single_post(
        self,
        post_index: int,
        batch_dir: str,
        retry_count: int = 2
    ) -> Dict:
        """
        æ”¶é›†å•ä¸ªå¸–å­è¯¦æƒ…ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        Args:
            post_index: å¸–å­åºå·
            batch_dir: ä¿å­˜ç›®å½•
            retry_count: é‡è¯•æ¬¡æ•°
        """
        for attempt in range(retry_count + 1):
            try:
                detail_task = f"""
                ç‚¹å‡»ç¬¬ {post_index} ä¸ªå¸–å­ï¼Œä½¿ç”¨ extract_structured_data æ”¶é›†ï¼š

                å¸–å­ä¿¡æ¯ï¼š
                - title: æ ‡é¢˜
                - author: ä½œè€…
                - publish_time: å‘å¸ƒæ—¶é—´
                - likes: ç‚¹èµ
                - collections: æ”¶è—
                - comments_count: è¯„è®ºæ•°
                - content: å†…å®¹
                - tags: æ ‡ç­¾æ•°ç»„

                è¯„è®ºä¿¡æ¯ï¼ˆå‰10æ¡ï¼‰ï¼š
                - top_comments æ•°ç»„ï¼Œæ¯æ¡åŒ…å«ï¼š
                  - nickname: æ˜µç§°
                  - content: å†…å®¹
                  - likes: ç‚¹èµ
                  - time: æ—¶é—´

                å®Œæˆåè¿”å›åˆ—è¡¨é¡µ
                """

                detail_agent = Agent(
                    task=detail_task,
                    llm=self.llm,
                    browser_context=self.context,
                    use_vision=self.use_vision
                )

                detail_result = await detail_agent.run()

                # æå–æ•°æ®
                post_data = None
                for content in detail_result.extracted_content():
                    extracted = self.extract_json_from_text(str(content))
                    if extracted:
                        post_data = extracted
                        break

                if not post_data:
                    if attempt < retry_count:
                        print(f"  âš ï¸  ç¬¬ {post_index} ä¸ªå¸–å­æ•°æ®æå–å¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{retry_count}...")
                        await asyncio.sleep(2)
                        continue
                    else:
                        post_data = {"error": "æœªæå–åˆ°æ•°æ®", "attempts": attempt + 1}

                # ä¿å­˜æ•°æ®
                detail_file = f"{batch_dir}/post_{post_index}.json"
                with open(detail_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "post_index": post_index,
                        "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "data": post_data,
                        "attempts": attempt + 1
                    }, f, ensure_ascii=False, indent=2)

                return post_data

            except Exception as e:
                if attempt < retry_count:
                    print(f"  âš ï¸  ç¬¬ {post_index} ä¸ªå¸–å­æ”¶é›†å‡ºé”™ï¼Œé‡è¯• {attempt + 1}/{retry_count}: {str(e)}")
                    await asyncio.sleep(2)
                else:
                    print(f"  âŒ ç¬¬ {post_index} ä¸ªå¸–å­æ”¶é›†å¤±è´¥: {str(e)}")
                    # ä¿å­˜é”™è¯¯ä¿¡æ¯
                    detail_file = f"{batch_dir}/post_{post_index}.json"
                    with open(detail_file, 'w', encoding='utf-8') as f:
                        json.dump({
                            "post_index": post_index,
                            "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            "error": str(e),
                            "attempts": attempt + 1
                        }, f, ensure_ascii=False, indent=2)
                    return {"error": str(e)}

        return {"error": "æœªçŸ¥é”™è¯¯"}

    async def collect_posts_sequential(self, posts_list: List[Dict], batch_dir: str):
        """é¡ºåºæ”¶é›†å¸–å­è¯¦æƒ…"""
        print(f"ğŸ“ æ­¥éª¤2: é¡ºåºæ”¶é›†å¸–å­è¯¦æƒ…...\n")

        for i in range(1, min(self.max_posts, len(posts_list)) + 1):
            print(f"  [{i}/{self.max_posts}] æ”¶é›†ç¬¬ {i} ä¸ªå¸–å­...")
            await self.collect_single_post(i, batch_dir)
            print(f"  âœ… ç¬¬ {i} ä¸ªå¸–å­æ”¶é›†å®Œæˆ\n")
            await asyncio.sleep(1)

    async def collect_posts_concurrent(self, posts_list: List[Dict], batch_dir: str):
        """
        å¹¶å‘æ”¶é›†å¸–å­è¯¦æƒ…
        å€Ÿé‰´ vibetest çš„å¹¶å‘æ¨¡å¼ï¼Œä½†ä½¿ç”¨å…±äº« context
        """
        print(f"ğŸ“ æ­¥éª¤2: å¹¶å‘æ”¶é›†å¸–å­è¯¦æƒ…ï¼ˆæœ€å¤§å¹¶å‘æ•°: {self.max_concurrent}ï¼‰...\n")

        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def collect_with_semaphore(post_index: int):
            async with semaphore:
                print(f"  ğŸ”„ å¼€å§‹æ”¶é›†ç¬¬ {post_index} ä¸ªå¸–å­...")
                result = await self.collect_single_post(post_index, batch_dir)
                print(f"  âœ… ç¬¬ {post_index} ä¸ªå¸–å­æ”¶é›†å®Œæˆ")
                return result

        # å¹¶å‘æ”¶é›†
        tasks = [
            collect_with_semaphore(i)
            for i in range(1, min(self.max_posts, len(posts_list)) + 1)
        ]

        await asyncio.gather(*tasks, return_exceptions=True)
        print()

    async def collect_posts(self):
        """ä¸»æ”¶é›†æµç¨‹"""
        # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡ï¼ˆå§‹ç»ˆå¯è§ï¼‰
        if self.context is None:
            self.context = await self.browser.new_context(
                config=BrowserContextConfig(
                    headless=False,  # å§‹ç»ˆæ˜¾ç¤ºæµè§ˆå™¨
                )
            )

        # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        batch_dir = f"{self.output_dir}/batch_{timestamp}"
        os.makedirs(batch_dir, exist_ok=True)

        print(f"\n{'='*60}")
        print(f"å°çº¢ä¹¦å¸–å­æ”¶é›†å™¨ - ä¼˜åŒ–ç‰ˆ")
        print(f"{'='*60}")
        print(f"ç›®æ ‡é¡µé¢: {self.xiaohongshu_url}")
        print(f"æ”¶é›†æ•°é‡: {self.max_posts} ä¸ªå¸–å­")
        print(f"ä¿å­˜ç›®å½•: {batch_dir}")
        print(f"æ¨¡å¼: {'å¹¶å‘' if self.concurrent else 'é¡ºåº'}")
        print(f"è§†è§‰æ¨¡å¼: {'å¼€å¯' if self.use_vision else 'å…³é—­'}")
        print(f"æµè§ˆå™¨: å¯è§çª—å£")
        print(f"{'='*60}\n")

        try:
            # Scout æ¢æµ‹
            scout_data = await self.scout_posts()

            # ä¿å­˜ Scout æŠ¥å‘Š
            scout_file = f"{batch_dir}/scout_report.json"
            with open(scout_file, 'w', encoding='utf-8') as f:
                json.dump(scout_data, f, ensure_ascii=False, indent=2)

            # æ”¶é›†å¸–å­åˆ—è¡¨
            posts_list = await self.collect_post_list()

            # ä¿å­˜å¸–å­åˆ—è¡¨
            list_file = f"{batch_dir}/posts_list.json"
            with open(list_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "total": len(posts_list),
                    "posts": posts_list
                }, f, ensure_ascii=False, indent=2)

            # æ”¶é›†è¯¦æƒ…ï¼ˆé¡ºåºæˆ–å¹¶å‘ï¼‰
            if self.concurrent:
                await self.collect_posts_concurrent(posts_list, batch_dir)
            else:
                await self.collect_posts_sequential(posts_list, batch_dir)

            # ä¿å­˜æ±‡æ€»ä¿¡æ¯
            summary_file = f"{batch_dir}/summary.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "url": self.xiaohongshu_url,
                    "total_posts": self.max_posts,
                    "output_dir": batch_dir,
                    "mode": "concurrent" if self.concurrent else "sequential",
                    "use_vision": self.use_vision,
                    "headless": False
                }, f, ensure_ascii=False, indent=2)

            print(f"\n{'='*60}")
            print(f"âœ… æ”¶é›†å®Œæˆï¼")
            print(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {batch_dir}")
            print(f"{'='*60}\n")

        except Exception as e:
            print(f"\nâŒ æ”¶é›†è¿‡ç¨‹å‡ºé”™: {str(e)}")
            raise

        finally:
            # æ¸…ç†èµ„æºï¼ˆå€Ÿé‰´ vibetest çš„ä¸¥æ ¼èµ„æºç®¡ç†ï¼‰
            if self.context:
                try:
                    await self.context.close()
                except Exception:
                    pass

            try:
                await self.browser.close()
            except Exception:
                pass

            # ç­‰å¾…èµ„æºå®Œå…¨é‡Šæ”¾
            await asyncio.sleep(1)


async def main():
    """
    ä¸»å‡½æ•°

    æ–°åŠŸèƒ½ï¼š
    1. Scout æ¨¡å¼è‡ªåŠ¨æ¢æµ‹é¡µé¢
    2. å¯é€‰å¹¶å‘æ”¶é›†ï¼ˆæé«˜é€Ÿåº¦ï¼‰
    3. æ€§èƒ½ä¼˜åŒ–çš„æµè§ˆå™¨é…ç½®
    4. æ›´å¥½çš„é”™è¯¯å¤„ç†
    5. å§‹ç»ˆæ˜¾ç¤ºæµè§ˆå™¨çª—å£
    """

    # ============================================================
    # é…ç½®åŒºåŸŸ
    # ============================================================

    xiaohongshu_url = "https://www.xiaohongshu.com/explore"
    max_posts = 3

    # é…ç½®é€‰é¡¹
    use_vision = False        # æ˜¯å¦å¯ç”¨è§†è§‰æ¨¡å¼ï¼ˆæ˜¾ç¤ºå…ƒç´ æ ‡è¯†ï¼‰
    concurrent = False        # æ˜¯å¦å¹¶å‘æ”¶é›†ï¼ˆTrue = æ›´å¿«ä½†å ç”¨æ›´å¤šèµ„æºï¼‰
    max_concurrent = 2        # æœ€å¤§å¹¶å‘æ•°ï¼ˆä»…åœ¨ concurrent=True æ—¶æœ‰æ•ˆï¼‰

    # ============================================================
    # åˆ›å»ºå¹¶è¿è¡Œæ”¶é›†å™¨
    # ============================================================

    collector = XiaohongshuCollectorOptimized(
        xiaohongshu_url=xiaohongshu_url,
        max_posts=max_posts,
        use_vision=use_vision,
        concurrent=concurrent,
        max_concurrent=max_concurrent
    )

    await collector.collect_posts()


if __name__ == "__main__":
    asyncio.run(main())
